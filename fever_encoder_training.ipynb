{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer, losses, BiSentenceTransformer\n",
    "from sentence_transformers.readers import STSDataReader, FEVERReader\n",
    "from sentence_transformers.datasets import SentencesDataset\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "import psycopg2\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
    "train_batch_size = 16\n",
    "num_epochs = 1\n",
    "warmup_steps=100\n",
    "model_save_path='./fever-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = FEVERReader()\n",
    "train_examples = reader.get_examples('train',table='test.train_article_rerank')\n",
    "train_data = SentencesDataset(train_examples, base_model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=base_model)\n",
    "\n",
    "dev_examples = reader.get_examples('dev',table='test.test_article_rerank')\n",
    "dev_data = SentencesDataset(examples=dev_examples, model=base_model)\n",
    "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path,\n",
    "          fp16=True,\n",
    "          fp16_opt_level='O1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Modified Siamese (dense feedforward layer for query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiSentenceTransformer(base_model)\n",
    "train_batch_size = 16\n",
    "num_epochs = 1\n",
    "warmup_steps=100\n",
    "model_save_path='./modified-fever'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to connect to postgres...\n",
      "connected to postgres\n",
      "downloading data\n",
      "trying to connect to postgres...\n",
      "connected to postgres\n",
      "downloading data\n"
     ]
    }
   ],
   "source": [
    "reader = FEVERReader()\n",
    "train_examples = reader.get_examples('train',table='test.train_article_rerank')\n",
    "train_data = SentencesDataset(train_examples, base_model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.BiCosineSimilarityLoss(model=model)\n",
    "\n",
    "dev_examples = reader.get_examples('dev',table='test.test_article_rerank')\n",
    "dev_data = SentencesDataset(examples=dev_examples, model=base_model)\n",
    "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'cosine_similiarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-91a4174bb9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0moutput_path_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           \u001b[0mfp16_opt_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'O1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/sentence-transformers/sentence_transformers/BiSentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objective, evaluator, epochs, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path_base, save_best_model, max_grad_norm, fp16, fp16_opt_level, local_rank)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similiarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfp16_opt_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similiarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.functional' has no attribute 'cosine_similiarity'"
     ]
    }
   ],
   "source": [
    "model.fit((train_dataloader, train_loss),\n",
    "          None,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path_base=model_save_path,\n",
    "          fp16=True,\n",
    "          fp16_opt_level='O1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = './modified-fever'\n",
    "base_model = SentenceTransformer(load_path)\n",
    "loaded_model = BiSentenceTransformer(base_model, path=load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode document set and dump to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_path = './modified-fever/'\n",
    "model = BiSentenceTransformer(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to connect...\n",
      "executing query...\n"
     ]
    }
   ],
   "source": [
    "HOST = '54.196.150.193'\n",
    "USER = 'postgres'\n",
    "PASS = os.environ.get('PGPASS')\n",
    "PGSSLROOTCERT = os.environ.get('PGSSLROOTCERT')\n",
    "if PASS == None or PGSSLROOTCERT == None:\n",
    "    print(\"Please set PG_PASS and PGSSLROOTCERT env variable\")\n",
    "    raise SystemExit()\n",
    "DBNAME = 'fever'\n",
    "POSTGRES_DSN = f'''dbname='fever' user='{USER}' host='{HOST}' password='{PASS}' '''\n",
    "query = '''\n",
    "select a.id, l.text \n",
    "from wiki.articles_clean a\n",
    "join wiki.lines l on l.article_id = a.id and line_number = 0\n",
    "'''\n",
    "print('trying to connect...')\n",
    "conn = psycopg2.connect(POSTGRES_DSN)\n",
    "cur = conn.cursor()\n",
    "print('executing query...')\n",
    "cur.execute(query)\n",
    "res = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_buffer = []\n",
    "ids_buffer = []\n",
    "BATCH_SIZE = 100000\n",
    "if not os.path.exists('./fever-embs/'):\n",
    "    os.makedirs('./fever-embs/')\n",
    "start = time.time()\n",
    "for i, batch in enumerate(res):\n",
    "    sent_buffer.append(batch[1])\n",
    "    ids_buffer.append(batch[0])\n",
    "    if (i+1) % BATCH_SIZE == 0:\n",
    "        embs = model.model_b.encode(sent_buffer, batch_size=32)\n",
    "        ids_buffer = np.array(ids_buffer)\n",
    "        ids_buffer = np.expand_dims(ids_buffer, 1)\n",
    "        to_save = np.concatenate((ids_buffer, embs), 1)\n",
    "        np.save(f'./fever-embs/emb-{i}',to_save)\n",
    "        sent_buffer = []\n",
    "        ids_buffer = []\n",
    "        print(f'Running {i/(time.time() - start)} per second')\n",
    "embs = model.model_b.encode(sent_buffer, batch_size=32)\n",
    "ids_buffer = np.array(ids_buffer)\n",
    "ids_buffer = np.expand_dims(ids_buffer, 1)\n",
    "to_save = np.concatenate((ids_buffer, embs), 1)\n",
    "np.save(f'./fever-embs/emb-{i}',to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embs = model.encode(sent_buffer, batch_size=32)\n",
    "#ids_buffer = np.array(ids_buffer)\n",
    "#ids_buffer = np.expand_dims(ids_buffer, 1)\n",
    "#to_save = np.concatenate((ids_buffer, embs), 1)\n",
    "np.save(f'./fever-embs/emb-last',to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
